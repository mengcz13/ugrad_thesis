\chapter{引言}
\label{cha:intro}

\section{研究动机}
近年来，固态硬盘（SSD，Solid-State Drive）逐渐成为各个领域占统治地位的存储器。与传统磁盘相比，固态硬盘因为没有机械结构，具有随机访问性能优异，抗震性好，噪音小，节能等优点，在个人电脑、云服务和高性能计算场景中得到了越来越广泛的应用。然而，固态硬盘自身仍然存在限制其性能进一步提高的问题。尽管固态硬盘内部的闪存芯片能够提供高性能的读写，固态硬盘用于管理这些芯片的主控程序依然不能在所有场景下充分发挥它们的性能\cite{bjorling_lightnvm:_nodate}。

虽然固态硬盘的内部结构与传统块设备大不相同，其搭载的主控却能够允许操作系统像操作普通块设备一样操作固态硬盘，而主控则负责将操作系统的请求转换为对固态硬盘内部芯片的读、写和擦除操作。遗憾的是，长期以来固态硬盘内部的主控算法都由其内部的控制芯片执行，操作系统和应用程序无法得知其内部的操作细节，从而丧失了优化的机会。随着开放通道SSD的出现，这一问题迎来了解决的希望。开放通道SSD将内部结构暴露给外部应用程序，使得操作系统和应用程序参与SSD内部芯片的管理和承担一部分主控的工作成为可能。目前，一些顶级的云计算服务商已开始采用开放通道SSD改进系统性能\cite{wang_efficient_2014}，也已经有一些工作尝试在RocksDB数据库等应用上面向开放通道SSD优化其IO性能\cite{gonzalez_towards_nodate}，但在高性能应用领域结合开放通道SSD和应用负载特点进行IO优化的工作还不很多。

高性能应用的IO有其独特的模式和特征，例如大块IO较多、顺序访问占绝大多数、仅添加（append-only）类型的写入为主要写入类型等\cite{shan_characterizing_2008}。与常见的操作系统和应用程序产生的IO序列相比，高性能应用的IO序列对固态硬盘的访问更为友好，这为简化SSD内部芯片管理机制、降低管理开销提供了优化的空间。

遵循这一思路，本论文分析了两种高性能应用的IO负载特征，并基于这些特征尝试在开放通道SSD上改进闪存管理策略，所提出的方法在保证高性能应用IO性能的前提下成功降低了闪存管理的开销。

\section{相关工作}
在开放通道SSD和闪存设备方面，OFTL\cite{lu_extending_nodate}在开放通道SSD上通过实现基于对象的闪存转换层减少写入放大同时延长设备的使用寿命。uFLIP\cite{bouganim_uflip:_nodate}和uFLIP-OC\cite{picoli_uflip-oc:_2017}分别针对普通SSD设备和开放通道SSD设备设计了用于评测IO性能的多种序列，评估了所用设备面对不同类型IO负载时的性能表现。LightNVM\cite{bjorling_lightnvm:_nodate}作为Linux上的开放通道SSD子系统，为Linux内核提供了开放通道SSD支持，同时提供了一个开源的闪存转换层实现pblk，和一个用于操作开放通道SSD的用户态库liblightnvm。其中pblk实现了读写分离、可预测的IO延迟和IO调度功能，并在多种应用场景下表现优异\cite{bjorling_open-channel_2017}。He等人的工作\cite{he_unwritten_2017}通过分析来自真实负载的IO Trace，揭示了希望在SSD上获得较高读写性能时必须遵守的规则。

在高性能应用的IO负载评测方面，Huong等人的工作\cite{luu_multi-level_nodate}创建了一个能够在多个层面跟踪IO的框架，进而快速确定应用出现IO性能低效的原因。他们的另一项工作\cite{luu_multiplatform_2015}通过分析数以千计的高性能应用的IO行为，总结了有利于提高应用吞吐量的措施。Liu等人的工作\cite{liu_performance_2017}，研究了非易失性存储器引入后对高性能应用IO性能的影响：缓慢存储造成的性能问题得到缓解，且缩小存储器和CPU之间性能差距的机制可能是不必要的。

在基于开放通道SSD优化特定应用的性能表现方面，大部分工作集中于数据库系统。Lee\cite{lee_case_2008}通过实证方法测试了现有数据库对闪存的利用情况，并证明在事务日志、回滚段和临时表空间替换等环节换用SSD能带来大幅度的性能提升。Gonzalez\cite{gonzalez_towards_nodate}针对RocksDB数据库的特点为其设计了直接读写开放通道SSD的IO后端，优化了其IO性能。Wang\cite{wang_efficient_2014}提出了一种能更好地利用SSD并行性的FOCS系统改善了基于日志结构的合并树（LSM-Tree）的性能。

\section{主要工作}
本文主要研究的问题是基于开放通道SSD，针对高性能应用的IO负载特征对其IO性能进行优化，主要工作如下：
\begin{itemize}
    \item 抓取了典型高性能计算应用LAMMPS和MACDRP的IO负载记录并分析了二者的负载特征，发现其负载特征具有绝大部分请求按页对齐、单次请求数据量恰好覆盖整数个页大小、覆盖写比例很高的特点；
    \item 在用户态实现了多种IO优化策略与相应的评测程序，通过吞吐量、映射表维护开销、擦除次数和写放大系数等指标评价不同IO优化策略的优劣；
    \item 根据高性能计算应用的IO负载特征，针对目前已有闪存转换层的映射方式进行改进，通过将映射粒度从页级别提高到超级块级别，并引入日志块实现混合映射以应对块不对齐和小于块大小的写入，在保证IO性能不下降的同时大幅减少了映射表体积和维护开销。
\end{itemize}

本文接下来各章的主要内容如下：第2章介绍了SSD读写的特性与开放通道SSD的特点，然后分析了两种高性能应用LAMMPS和MACDRP的负载特性；第3章按照目前已有的闪存转换层所采用的映射与垃圾回收方法实现了基本的IO优化策略，然后根据高性能应用的负载特征分别从垃圾回收方式与映射方式两方面提出改进的IO优化策略；第4章通过比较基本优化策略与改进的几种优化策略在重放高性能应用IO负载时的性能指标，包括吞吐量、映射表维护成本、擦除次数和写放大系数，证明采用混合映射的改进策略能够在保持IO性能的同时减少映射表大小和维护成本。第5章总结了本文的工作并列出了今后需要继续改进的方向。